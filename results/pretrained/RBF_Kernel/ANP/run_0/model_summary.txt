AttnLNP(
  (x_encoder): MLP(
    (dropout): Identity()
    (activation): ReLU()
    (to_hidden): Linear(in_features=1, out_features=128, bias=True)
    (linears): ModuleList()
    (out): Linear(in_features=128, out_features=128, bias=True)
  )
  (decoder): MergeFlatInputs(
    (resizer): MLP(
      (dropout): Identity()
      (activation): ReLU()
      (to_hidden): Linear(in_features=128, out_features=128, bias=True)
      (linears): ModuleList()
      (out): Linear(in_features=128, out_features=128, bias=True)
    )
    (flat_module): MLP(
      (dropout): Identity()
      (activation): ReLU()
      (to_hidden): Linear(in_features=128, out_features=128, bias=True)
      (linears): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (out): Linear(in_features=128, out_features=2, bias=True)
    )
  )
  (p_y_loc_transformer): Identity()
  (xy_encoder): MergeFlatInputs(
    (resizer): MLP(
      (dropout): Identity()
      (activation): ReLU()
      (to_hidden): Linear(in_features=1, out_features=32, bias=True)
      (linears): ModuleList()
      (out): Linear(in_features=32, out_features=128, bias=True)
    )
    (flat_module): MLP(
      (dropout): Identity()
      (activation): ReLU()
      (to_hidden): Linear(in_features=128, out_features=128, bias=True)
      (linears): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
      (out): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (attender): TransformerAttender(
    (key_transform): Linear(in_features=128, out_features=128, bias=False)
    (query_transform): Linear(in_features=128, out_features=128, bias=True)
    (value_transform): Linear(in_features=128, out_features=128, bias=False)
    (dot): DotAttender(
      (dropout): Identity()
    )
    (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (mlp): MLP(
      (dropout): Identity()
      (activation): ReLU()
      (to_hidden): Linear(in_features=128, out_features=128, bias=True)
      (linears): ModuleList()
      (out): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (latent_encoder): MLP(
    (dropout): Identity()
    (activation): ReLU()
    (to_hidden): Linear(in_features=128, out_features=128, bias=True)
    (linears): ModuleList()
    (out): Linear(in_features=128, out_features=256, bias=True)
  )
  (r_z_merger): Linear(in_features=256, out_features=128, bias=True)
  (q_z_loc_transformer): Identity()
)